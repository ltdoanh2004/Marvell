{
  "model": "gpt-4o-mini",
  "dataset": "/Users/doa_ai/Developer/Marvell/AgenticMemory/data/locomo10.json",
  "total_questions": 199,
  "category_distribution": {
    "2": 37,
    "3": 13,
    "1": 32,
    "4": 70,
    "5": 47
  },
  "aggregate_metrics": {
    "overall": {
      "exact_match": {
        "mean": 0.10552763819095477,
        "std": 0.3080069492688908,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 199
      },
      "f1": {
        "mean": 0.3028872253178567,
        "std": 0.3613120297006424,
        "median": 0.125,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge1_f": {
        "mean": 0.31330569500423977,
        "std": 0.3587132620768955,
        "median": 0.18181818181818182,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge2_f": {
        "mean": 0.2169938417462005,
        "std": 0.3526375316324558,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rougeL_f": {
        "mean": 0.3045404825892123,
        "std": 0.35787549723495726,
        "median": 0.15999999999999998,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "bleu1": {
        "mean": 0.26032069861239965,
        "std": 0.33392516633266234,
        "median": 0.11111111111111109,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu2": {
        "mean": 0.20340294362509737,
        "std": 0.32148921545812814,
        "median": 0.03321055820775358,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu3": {
        "mean": 0.16975716783085512,
        "std": 0.30242253421382675,
        "median": 0.021746357799665227,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu4": {
        "mean": 0.140221093464013,
        "std": 0.2671636172633397,
        "median": 0.017033186037639283,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bert_precision": {
        "mean": 0.888343947317133,
        "std": 0.05539875429102072,
        "median": 0.8730800151824951,
        "min": 0.7912019491195679,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_recall": {
        "mean": 0.8890134794029159,
        "std": 0.0629881999792877,
        "median": 0.8825975656509399,
        "min": 0.7486652135848999,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_f1": {
        "mean": 0.888231969358933,
        "std": 0.056149484097245614,
        "median": 0.868986189365387,
        "min": 0.7911079525947571,
        "max": 1.000000238418579,
        "count": 199
      },
      "meteor": {
        "mean": 0.24110634449257165,
        "std": 0.3402456072876566,
        "median": 0.06329113924050632,
        "min": 0.0,
        "max": 0.9996243425995492,
        "count": 199
      },
      "sbert_similarity": {
        "mean": 0.48108270498058153,
        "std": 0.33175732087854065,
        "median": 0.4493759274482727,
        "min": -0.048551272600889206,
        "max": 1.0000001192092896,
        "count": 199
      }
    },
    "category_1": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0.0,
        "min": 0,
        "max": 0,
        "count": 32
      },
      "f1": {
        "mean": 0.14082096426175866,
        "std": 0.20509397634374382,
        "median": 0.0,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 32
      },
      "rouge1_f": {
        "mean": 0.14824756907567166,
        "std": 0.20559030831374164,
        "median": 0.0,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 32
      },
      "rouge2_f": {
        "mean": 0.031274975024975026,
        "std": 0.10588663607223812,
        "median": 0.0,
        "min": 0.0,
        "max": 0.5714285714285715,
        "count": 32
      },
      "rougeL_f": {
        "mean": 0.12540712192120013,
        "std": 0.1757155878253191,
        "median": 0.0,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 32
      },
      "bleu1": {
        "mean": 0.1022481052048459,
        "std": 0.140343891070262,
        "median": 0.0,
        "min": 0,
        "max": 0.5555555555555556,
        "count": 32
      },
      "bleu2": {
        "mean": 0.04961208522822125,
        "std": 0.09004103519246823,
        "median": 0.0,
        "min": 0,
        "max": 0.372677996249965,
        "count": 32
      },
      "bleu3": {
        "mean": 0.03773877507570815,
        "std": 0.07412014489184632,
        "median": 0.0,
        "min": 0,
        "max": 0.301194211912202,
        "count": 32
      },
      "bleu4": {
        "mean": 0.027553528768577883,
        "std": 0.05844819229943942,
        "median": 0.0,
        "min": 0,
        "max": 0.301194211912202,
        "count": 32
      },
      "bert_precision": {
        "mean": 0.8565245885401964,
        "std": 0.03993660756155295,
        "median": 0.8531852662563324,
        "min": 0.8005184531211853,
        "max": 0.9644248485565186,
        "count": 32
      },
      "bert_recall": {
        "mean": 0.8562487978488207,
        "std": 0.04153834570726948,
        "median": 0.8470431268215179,
        "min": 0.7900184988975525,
        "max": 0.9472923874855042,
        "count": 32
      },
      "bert_f1": {
        "mean": 0.8558676894754171,
        "std": 0.034533251826159635,
        "median": 0.8485055863857269,
        "min": 0.8060428500175476,
        "max": 0.9329448938369751,
        "count": 32
      },
      "meteor": {
        "mean": 0.0649857010498655,
        "std": 0.12141094421790345,
        "median": 0.0,
        "min": 0.0,
        "max": 0.48076923076923084,
        "count": 32
      },
      "sbert_similarity": {
        "mean": 0.3841902771964669,
        "std": 0.22447754362591088,
        "median": 0.3341367691755295,
        "min": 0.09131599962711334,
        "max": 0.8449621200561523,
        "count": 32
      }
    },
    "category_2": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 37
      },
      "f1": {
        "mean": 0.35980460980460977,
        "std": 0.32204121001946423,
        "median": 0.3636363636363636,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "rouge1_f": {
        "mean": 0.35980460980460977,
        "std": 0.32204121001946423,
        "median": 0.3636363636363636,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "rouge2_f": {
        "mean": 0.2425782925782926,
        "std": 0.2727843071865702,
        "median": 0.22222222222222224,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "rougeL_f": {
        "mean": 0.35980460980460977,
        "std": 0.32204121001946423,
        "median": 0.3636363636363636,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "bleu1": {
        "mean": 0.265297350647484,
        "std": 0.24336377762326966,
        "median": 0.25,
        "min": 0,
        "max": 0.75,
        "count": 37
      },
      "bleu2": {
        "mean": 0.15794253831064073,
        "std": 0.16335955572001606,
        "median": 0.09128709291752769,
        "min": 0,
        "max": 0.5163977794943222,
        "count": 37
      },
      "bleu3": {
        "mean": 0.0839985100152133,
        "std": 0.09804806955542775,
        "median": 0.06036431606009924,
        "min": 0,
        "max": 0.4091569047284194,
        "count": 37
      },
      "bleu4": {
        "mean": 0.059007800515376865,
        "std": 0.06055289547935061,
        "median": 0.05372849659117709,
        "min": 0,
        "max": 0.21711852081087685,
        "count": 37
      },
      "bert_precision": {
        "mean": 0.8964817523956299,
        "std": 0.046061582814319706,
        "median": 0.9072569012641907,
        "min": 0.7912019491195679,
        "max": 0.9594281315803528,
        "count": 37
      },
      "bert_recall": {
        "mean": 0.8949211690876935,
        "std": 0.061246668030567185,
        "median": 0.9111441373825073,
        "min": 0.780808687210083,
        "max": 0.9838892221450806,
        "count": 37
      },
      "bert_f1": {
        "mean": 0.895029142096236,
        "std": 0.04868493094247127,
        "median": 0.9048651456832886,
        "min": 0.7974472641944885,
        "max": 0.9715046882629395,
        "count": 37
      },
      "meteor": {
        "mean": 0.1630644963352661,
        "std": 0.20598317157012452,
        "median": 0.08620689655172413,
        "min": 0.0,
        "max": 0.866013071895425,
        "count": 37
      },
      "sbert_similarity": {
        "mean": 0.6388657628684431,
        "std": 0.26690189245924745,
        "median": 0.7072815299034119,
        "min": 0.14799773693084717,
        "max": 0.9868618845939636,
        "count": 37
      }
    },
    "category_3": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 13
      },
      "f1": {
        "mean": 0.08061396982801665,
        "std": 0.07515598659452559,
        "median": 0.08695652173913043,
        "min": 0.0,
        "max": 0.2222222222222222,
        "count": 13
      },
      "rouge1_f": {
        "mean": 0.1197859380438566,
        "std": 0.09464789357531991,
        "median": 0.13333333333333333,
        "min": 0.0,
        "max": 0.23529411764705882,
        "count": 13
      },
      "rouge2_f": {
        "mean": 0.006153846153846155,
        "std": 0.022188007849009168,
        "median": 0.0,
        "min": 0.0,
        "max": 0.08000000000000002,
        "count": 13
      },
      "rougeL_f": {
        "mean": 0.10793408619200474,
        "std": 0.0894192348488987,
        "median": 0.12500000000000003,
        "min": 0.0,
        "max": 0.23529411764705882,
        "count": 13
      },
      "bleu1": {
        "mean": 0.08583562031837894,
        "std": 0.06841018852787142,
        "median": 0.0909090909090909,
        "min": 0,
        "max": 0.2,
        "count": 13
      },
      "bleu2": {
        "mean": 0.03069500179796045,
        "std": 0.028544431166869794,
        "median": 0.03015113445777637,
        "min": 0,
        "max": 0.09759000729485329,
        "count": 13
      },
      "bleu3": {
        "mean": 0.01875297124633739,
        "std": 0.015826167438816683,
        "median": 0.02171882703825944,
        "min": 0,
        "max": 0.04506819749679463,
        "count": 13
      },
      "bleu4": {
        "mean": 0.014658189171210027,
        "std": 0.01278909674433295,
        "median": 0.013686241957588646,
        "min": 0,
        "max": 0.040824829046386304,
        "count": 13
      },
      "bert_precision": {
        "mean": 0.8472536252095149,
        "std": 0.019972134222378805,
        "median": 0.8527673482894897,
        "min": 0.8052605390548706,
        "max": 0.8731280565261841,
        "count": 13
      },
      "bert_recall": {
        "mean": 0.8615730267304641,
        "std": 0.030341378560634777,
        "median": 0.8526804447174072,
        "min": 0.8179011940956116,
        "max": 0.910496711730957,
        "count": 13
      },
      "bert_f1": {
        "mean": 0.8540653861485995,
        "std": 0.01976550687534424,
        "median": 0.852533757686615,
        "min": 0.8216133117675781,
        "max": 0.8914209604263306,
        "count": 13
      },
      "meteor": {
        "mean": 0.058992367927045854,
        "std": 0.07009024089523153,
        "median": 0.041666666666666664,
        "min": 0.0,
        "max": 0.20408163265306123,
        "count": 13
      },
      "sbert_similarity": {
        "mean": 0.3472034636025245,
        "std": 0.21626847126825005,
        "median": 0.31286975741386414,
        "min": 0.05247403308749199,
        "max": 0.6807616949081421,
        "count": 13
      }
    },
    "category_4": {
      "exact_match": {
        "mean": 0.04285714285714286,
        "std": 0.203997320649802,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 70
      },
      "f1": {
        "mean": 0.3327126821788414,
        "std": 0.33465339383683795,
        "median": 0.25,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge1_f": {
        "mean": 0.351838002574097,
        "std": 0.3297797271588242,
        "median": 0.26785714285714285,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge2_f": {
        "mean": 0.23036540687568394,
        "std": 0.3412685029627409,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rougeL_f": {
        "mean": 0.3395621613230498,
        "std": 0.3297214149904219,
        "median": 0.26785714285714285,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "bleu1": {
        "mean": 0.27946894655988413,
        "std": 0.3012495962123843,
        "median": 0.17444219066937122,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu2": {
        "mean": 0.2188225703057809,
        "std": 0.2951224280275825,
        "median": 0.06422285251880867,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu3": {
        "mean": 0.18654285114368313,
        "std": 0.27164666713726354,
        "median": 0.04506819749679463,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu4": {
        "mean": 0.15113830822867583,
        "std": 0.22694764754297886,
        "median": 0.0358368123357508,
        "min": 0,
        "max": 0.8931539818068694,
        "count": 70
      },
      "bert_precision": {
        "mean": 0.8852900326251983,
        "std": 0.0510353649889562,
        "median": 0.8725464046001434,
        "min": 0.8028200268745422,
        "max": 1.000000238418579,
        "count": 70
      },
      "bert_recall": {
        "mean": 0.8956056833267212,
        "std": 0.05790222797500881,
        "median": 0.8925405144691467,
        "min": 0.7486652135848999,
        "max": 1.000000238418579,
        "count": 70
      },
      "bert_f1": {
        "mean": 0.8900623304503305,
        "std": 0.0517670597594697,
        "median": 0.8793226182460785,
        "min": 0.7911079525947571,
        "max": 1.000000238418579,
        "count": 70
      },
      "meteor": {
        "mean": 0.31128395000640735,
        "std": 0.33393158431227365,
        "median": 0.15037593984962408,
        "min": 0.0,
        "max": 0.9896039603960397,
        "count": 70
      },
      "sbert_similarity": {
        "mean": 0.5000487232846873,
        "std": 0.29353623301725823,
        "median": 0.48096974194049835,
        "min": 0.0581410676240921,
        "max": 1.0000001192092896,
        "count": 70
      }
    },
    "category_5": {
      "exact_match": {
        "mean": 0.3829787234042553,
        "std": 0.491368607293056,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 47
      },
      "f1": {
        "mean": 0.3854818523153942,
        "std": 0.4896719550221081,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge1_f": {
        "mean": 0.38521836506159013,
        "std": 0.48982251576499225,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge2_f": {
        "mean": 0.3617021276595745,
        "std": 0.485687854441406,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rougeL_f": {
        "mean": 0.38521836506159013,
        "std": 0.48982251576499225,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "bleu1": {
        "mean": 0.3837699651069093,
        "std": 0.4907680720845262,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu2": {
        "mean": 0.3687044721663711,
        "std": 0.48253960257630424,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu3": {
        "mean": 0.34392133019349813,
        "std": 0.46453033865989474,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu4": {
        "mean": 0.2993352771157897,
        "std": 0.42440088813769467,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bert_precision": {
        "mean": 0.9195156262276021,
        "std": 0.06497799854697858,
        "median": 0.8842162489891052,
        "min": 0.8458929657936096,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_recall": {
        "mean": 0.9044423496469538,
        "std": 0.08008055193143235,
        "median": 0.8672590851783752,
        "min": 0.7694870829582214,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_f1": {
        "mean": 0.9116405197914611,
        "std": 0.07162131389370138,
        "median": 0.8687825798988342,
        "min": 0.8076215386390686,
        "max": 1.000000238418579,
        "count": 47
      },
      "meteor": {
        "mean": 0.3683075845217257,
        "std": 0.4761715481263436,
        "median": 0.0,
        "min": 0.0,
        "max": 0.9996243425995492,
        "count": 47
      },
      "sbert_similarity": {
        "mean": 0.43162299016926514,
        "std": 0.4559886957720447,
        "median": 0.13336865603923798,
        "min": -0.048551272600889206,
        "max": 1.0000001192092896,
        "count": 47
      }
    }
  },
  "individual_results": [
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ support group?",
      "prediction": "8 May, 2023",
      "reference": "7 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9594281315803528,
        "bert_recall": 0.9838892221450806,
        "bert_f1": 0.9715046882629395,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8955165147781372
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie paint a sunrise?",
      "prediction": "last year",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694186806678772,
        "bert_recall": 0.9111441373825073,
        "bert_f1": 0.8897925019264221,
        "meteor": 0.0,
        "sbert_similarity": 0.3672372102737427
      }
    },
    {
      "sample_id": 0,
      "question": "What fields would Caroline be likely to pursue in her educaton?",
      "prediction": "counseling or working in mental health",
      "reference": "Psychology, counseling certification",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8300347328186035,
        "bert_recall": 0.8526804447174072,
        "bert_f1": 0.8412051796913147,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.6807616949081421
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline research?",
      "prediction": "I went to a LGBTQ support group yesterday and it was so powerful.",
      "reference": "Adoption agencies",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.833297610282898,
        "bert_recall": 0.8247554898262024,
        "bert_f1": 0.8290045261383057,
        "meteor": 0.0,
        "sbert_similarity": 0.19959095120429993
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's identity?",
      "prediction": "Caroline is a trans woman.",
      "reference": "Transgender woman",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8668416738510132,
        "bert_recall": 0.8898980617523193,
        "bert_f1": 0.8782185912132263,
        "meteor": 0.0,
        "sbert_similarity": 0.5800904035568237
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie run a charity race?",
      "prediction": "last Saturday, 20 May 2023",
      "reference": "The sunday before 25 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.9251143932342529,
        "bert_recall": 0.9049964547157288,
        "bert_f1": 0.9149448871612549,
        "meteor": 0.3177966101694915,
        "sbert_similarity": 0.8413609862327576
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie planning on going camping?",
      "prediction": "next month",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8738219738006592,
        "bert_recall": 0.8416208624839783,
        "bert_f1": 0.8574191331863403,
        "meteor": 0.0,
        "sbert_similarity": 0.5679937601089478
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's relationship status?",
      "prediction": "The content does not explicitly mention Caroline's relationship status.",
      "reference": "Single",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8200023770332336,
        "bert_recall": 0.8391727805137634,
        "bert_f1": 0.8294768929481506,
        "meteor": 0.0,
        "sbert_similarity": 0.14358074963092804
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline give a speech at a school?",
      "prediction": "9 June, 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9279207587242126,
        "bert_recall": 0.882927417755127,
        "bert_f1": 0.9048651456832886,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8582825660705566
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline meet up with her friends, family, and mentors?",
      "prediction": "9 June, 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9279207587242126,
        "bert_recall": 0.882927417755127,
        "bert_f1": 0.9048651456832886,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8582825660705566
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Caroline had her current group of friends for?",
      "prediction": "4 years, since I moved from my home country.",
      "reference": "4 years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.13483997249264845,
        "bleu3": 0.06036431606009924,
        "bleu4": 0.03986357128268015,
        "bert_precision": 0.8473397493362427,
        "bert_recall": 0.9115403890609741,
        "bert_f1": 0.8782683610916138,
        "meteor": 0.1851851851851852,
        "sbert_similarity": 0.697969913482666
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Caroline move from 4 years ago?",
      "prediction": "The context does not provide information about where Caroline moved from 4 years ago.",
      "reference": "Sweden",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8036949634552002,
        "bert_recall": 0.8296650052070618,
        "bert_f1": 0.8164735436439514,
        "meteor": 0.0,
        "sbert_similarity": 0.12360282987356186
      }
    },
    {
      "sample_id": 0,
      "question": "How long ago was Caroline's 18th birthday?",
      "prediction": "ten years ago from 27 June, 2023",
      "reference": "10 years ago",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.812086820602417,
        "bert_recall": 0.9225872755050659,
        "bert_f1": 0.863817572593689,
        "meteor": 0.866013071895425,
        "sbert_similarity": 0.6634002923965454
      }
    },
    {
      "sample_id": 0,
      "question": "What career path has Caroline decided to persue?",
      "prediction": "Gonna continue my edu and check out career options",
      "reference": "counseling or mental health for Transgender people",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8096072673797607,
        "bert_recall": 0.802509605884552,
        "bert_f1": 0.8060428500175476,
        "meteor": 0.0,
        "sbert_similarity": 0.09131599962711334
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline still want to pursue counseling as a career if she hadn't received support growing up?",
      "prediction": "I realized how important it is for others to have a support system.",
      "reference": "Likely no",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.83042311668396,
        "bert_recall": 0.8179011940956116,
        "bert_f1": 0.8241146206855774,
        "meteor": 0.0,
        "sbert_similarity": 0.07001911103725433
      }
    },
    {
      "sample_id": 0,
      "question": "What activities does Melanie partake in?",
      "prediction": "pottery workshop, biking, camping, learning pottery",
      "reference": "pottery, camping, painting, swimming",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.5555555555555556,
        "bleu2": 0.372677996249965,
        "bleu3": 0.27428238865744764,
        "bleu4": 0.13485111859503685,
        "bert_precision": 0.909805953502655,
        "bert_recall": 0.9472923874855042,
        "bert_f1": 0.9281708002090454,
        "meteor": 0.11904761904761905,
        "sbert_similarity": 0.7747246623039246
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie sign up for a pottery class?",
      "prediction": "yesterday, 2 July, 2023",
      "reference": "2 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.5,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1384586850774637,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9072569012641907,
        "bert_recall": 0.9637253284454346,
        "bert_f1": 0.9346389770507812,
        "meteor": 0.32258064516129037,
        "sbert_similarity": 0.9025338292121887
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline going to the transgender conference?",
      "prediction": "this month",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705528378486633,
        "bert_recall": 0.8371521234512329,
        "bert_f1": 0.8535258173942566,
        "meteor": 0.0,
        "sbert_similarity": 0.48674145340919495
      }
    },
    {
      "sample_id": 0,
      "question": "Where has Melanie camped?",
      "prediction": "in the mountains",
      "reference": "beach, mountains, forest",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.9042640924453735,
        "bert_recall": 0.8439559936523438,
        "bert_f1": 0.8730698227882385,
        "meteor": 0.0,
        "sbert_similarity": 0.6089510917663574
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's kids like?",
      "prediction": "they keep me motivated",
      "reference": "dinosaurs, nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8484576940536499,
        "bert_recall": 0.8151729702949524,
        "bert_f1": 0.8314823508262634,
        "meteor": 0.0,
        "sbert_similarity": 0.14261677861213684
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the museum?",
      "prediction": "Not mentioned.",
      "reference": "5 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8906959295272827,
        "bert_recall": 0.8067461848258972,
        "bert_f1": 0.8466451168060303,
        "meteor": 0.0,
        "sbert_similarity": 0.14799773693084717
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline have a picnic?",
      "prediction": "not specified",
      "reference": "The week before 6 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8379316329956055,
        "bert_recall": 0.780808687210083,
        "bert_f1": 0.8083622455596924,
        "meteor": 0.0,
        "sbert_similarity": 0.1666514277458191
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline likely have Dr. Seuss books on her bookshelf?",
      "prediction": "I've got lots of kids' books- classics, stories from different cultures, educational books, all of that.",
      "reference": "Yes, since she collects classic children's books",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09090909090909091,
        "rouge1_f": 0.15999999999999998,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15999999999999998,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.020806259464411975,
        "bleu3": 0.013510398157576722,
        "bleu4": 0.010331208012220438,
        "bert_precision": 0.8653685450553894,
        "bert_recall": 0.8898049592971802,
        "bert_f1": 0.8774166703224182,
        "meteor": 0.0,
        "sbert_similarity": 0.5071792602539062
      }
    },
    {
      "sample_id": 0,
      "question": "What books has Melanie read?",
      "prediction": "\"Charlotte's Web\"",
      "reference": "\"Nothing is Impossible\", \"Charlotte's Web\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.301194211912202,
        "bleu2": 0.301194211912202,
        "bleu3": 0.301194211912202,
        "bleu4": 0.301194211912202,
        "bert_precision": 0.9526906609535217,
        "bert_recall": 0.9140009880065918,
        "bert_f1": 0.9329448938369751,
        "meteor": 0.39893617021276595,
        "sbert_similarity": 0.7511489391326904
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to destress?",
      "prediction": "pottery workshop; it was fun and therapeutic!",
      "reference": "Running, pottery",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.837725043296814,
        "bert_recall": 0.8449552059173584,
        "bert_f1": 0.8413246273994446,
        "meteor": 0.2,
        "sbert_similarity": 0.6121400594711304
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ conference?",
      "prediction": "10 July, 2023",
      "reference": "10 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.9584362506866455,
        "bert_recall": 0.982631266117096,
        "bert_f1": 0.9703829884529114,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.9868618845939636
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie read the book \"nothing is impossible\"?",
      "prediction": "not mentioned",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8321192264556885,
        "bert_recall": 0.9006904363632202,
        "bert_f1": 0.8650481104850769,
        "meteor": 0.0,
        "sbert_similarity": 0.21296735107898712
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline pursue writing as a career option?",
      "prediction": "Gonna continue my edu and check out career options.",
      "reference": "LIkely no; though she likes reading, she wants to be a counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8525310158729553,
        "bert_recall": 0.8428366780281067,
        "bert_f1": 0.8476561307907104,
        "meteor": 0.0,
        "sbert_similarity": 0.2707250714302063
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the adoption meeting?",
      "prediction": "Last Friday, 15 July, 2023.",
      "reference": "The friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727272,
        "rouge1_f": 0.7272727272727272,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.7272727272727272,
        "bleu1": 0.5,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.1083896554122787,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.9410030841827393,
        "bert_recall": 0.919663667678833,
        "bert_f1": 0.9302110075950623,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.8644826412200928
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the pottery workshop?",
      "prediction": "Last Fri, 14 July, 2023.",
      "reference": "The Friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.25,
        "bleu2": 0.059761430466719695,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.03303164318013808,
        "bert_precision": 0.9362829923629761,
        "bert_recall": 0.924565315246582,
        "bert_f1": 0.9303872585296631,
        "meteor": 0.0,
        "sbert_similarity": 0.6595144271850586
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered a member of the LGBTQ community?",
      "prediction": "It's great to see the love and support for the LGBTQ+ community.",
      "reference": "Likely no, she does not refer to herself as part of it",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913043,
        "rouge1_f": 0.16,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.033149677206589796,
        "bleu3": 0.021746357799665227,
        "bleu4": 0.0169861974906263,
        "bert_precision": 0.8611932992935181,
        "bert_recall": 0.8440467119216919,
        "bert_f1": 0.852533757686615,
        "meteor": 0.041666666666666664,
        "sbert_similarity": 0.05247403308749199
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in June?",
      "prediction": "two weekends ago",
      "reference": "The week before 27 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8117291331291199,
        "bert_recall": 0.7836591005325317,
        "bert_f1": 0.7974472641944885,
        "meteor": 0.0,
        "sbert_similarity": 0.3628024756908417
      }
    },
    {
      "sample_id": 0,
      "question": "What LGBTQ+ events has Caroline participated in?",
      "prediction": "Caroline has participated in a transgender conference, an LGBTQ conference, a school event, and is involved with the 'Connected LGBTQ Activists' group.",
      "reference": "Pride parade, school speech, support group",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.14814814814814814,
        "bleu2": 0.023870495801314433,
        "bleu3": 0.013742582324526931,
        "bleu4": 0.009871721786705418,
        "bert_precision": 0.8507933020591736,
        "bert_recall": 0.8785281181335449,
        "bert_f1": 0.8644382357597351,
        "meteor": 0.06578947368421054,
        "sbert_similarity": 0.3420775532722473
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to a pride parade during the summer?",
      "prediction": "last week on 3 July, 2023",
      "reference": "The week before 3 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.4000000000000001,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.3086066999241838,
        "bleu3": 0.12657485247324374,
        "bleu4": 0.08307018474412793,
        "bert_precision": 0.9398696422576904,
        "bert_recall": 0.95000159740448,
        "bert_f1": 0.9449084401130676,
        "meteor": 0.25,
        "sbert_similarity": 0.8780370950698853
      }
    },
    {
      "sample_id": 0,
      "question": "What events has Caroline participated in to help children?",
      "prediction": "I went to a LGBTQ support group yesterday and it was so powerful. I took the first step towards becoming a mom - I applied to adoption agencies! I went to a council meeting for adoption. It was inspiring and emotional - so many people wanted to create loving homes for children in need.",
      "reference": "Mentoring program, school speech",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8130975365638733,
        "bert_recall": 0.8179351091384888,
        "bert_f1": 0.8155090808868408,
        "meteor": 0.0,
        "sbert_similarity": 0.25487279891967773
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in July?",
      "prediction": "last week in July",
      "reference": "two weekends before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.15163266492815836,
        "bleu2": 0.05536842069051654,
        "bleu3": 0.04649251600872798,
        "bleu4": 0.04873039689743777,
        "bert_precision": 0.8443431854248047,
        "bert_recall": 0.8219099640846252,
        "bert_f1": 0.8329755663871765,
        "meteor": 0.08620689655172413,
        "sbert_similarity": 0.4880199432373047
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a mentorship program?",
      "prediction": "last weekend, 17 July 2023",
      "reference": "The weekend before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727272,
        "rouge1_f": 0.7272727272727272,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.7272727272727272,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5163977794943222,
        "bleu3": 0.4091569047284194,
        "bleu4": 0.21711852081087685,
        "bert_precision": 0.954475998878479,
        "bert_recall": 0.9471859931945801,
        "bert_f1": 0.9508170485496521,
        "meteor": 0.4990583804143125,
        "sbert_similarity": 0.9012303352355957
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie paint recently?",
      "prediction": "a painting of a sunflower on a canvas",
      "reference": "sunset",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8015376329421997,
        "bert_recall": 0.8515955209732056,
        "bert_f1": 0.8258086442947388,
        "meteor": 0.0,
        "sbert_similarity": 0.2863439917564392
      }
    },
    {
      "sample_id": 0,
      "question": "What activities has Melanie done with her family?",
      "prediction": "played games, ate good food, and just hung out together",
      "reference": "Pottery, painting, camping, museum, swimming, hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03892494720807617,
        "bleu3": 0.025677367760996977,
        "bleu4": 0.020255986027125642,
        "bert_precision": 0.8317267894744873,
        "bert_recall": 0.8155130743980408,
        "bert_f1": 0.8235400915145874,
        "meteor": 0.0,
        "sbert_similarity": 0.22525504231452942
      }
    },
    {
      "sample_id": 0,
      "question": "In what ways is Caroline participating in the LGBTQ community?",
      "prediction": "Caroline is participating by joining an LGBTQ activist group, attending support groups, sharing her transgender journey at school events, and creating art that supports the LGBTQ+ community.",
      "reference": "Joining activist group, going to pride parades, participating in an art show, mentoring program",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.29268292682926833,
        "rouge1_f": 0.29268292682926833,
        "rouge2_f": 0.05128205128205129,
        "rougeL_f": 0.1951219512195122,
        "bleu1": 0.2903225806451613,
        "bleu2": 0.13912166872805048,
        "bleu3": 0.08954662342482497,
        "bleu4": 0.039292384191895235,
        "bert_precision": 0.8721792101860046,
        "bert_recall": 0.8771071434020996,
        "bert_f1": 0.8746362328529358,
        "meteor": 0.2786855482933915,
        "sbert_similarity": 0.5353131294250488
      }
    },
    {
      "sample_id": 0,
      "question": "How many times has Melanie gone to the beach in 2023?",
      "prediction": "usually only once or twice a year.",
      "reference": 2,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8005184531211853,
        "bert_recall": 0.8601075410842896,
        "bert_f1": 0.8292438387870789,
        "meteor": 0.0,
        "sbert_similarity": 0.16021385788917542
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a new activist group?",
      "prediction": "last Tues, 20 July 2023",
      "reference": "The Tuesday before 20 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.5,
        "bleu2": 0.447213595499958,
        "bleu3": 0.3721003696282278,
        "bleu4": 0.20205155046766235,
        "bert_precision": 0.9317505955696106,
        "bert_recall": 0.9195437431335449,
        "bert_f1": 0.9256069660186768,
        "meteor": 0.4990583804143125,
        "sbert_similarity": 0.7302359342575073
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be more interested in going to a national park or a theme park?",
      "prediction": "Melanie appreciates the outdoors, mentioning how \"nature can be so refreshing for your soul\" and enjoys activities like \"biking\" and \"camping\".",
      "reference": "National park; she likes the outdoors",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.08000000000000002,
        "rougeL_f": 0.14814814814814814,
        "bleu1": 0.06896551724137931,
        "bleu2": 0.049629166698546515,
        "bleu3": 0.02171882703825944,
        "bleu4": 0.013686241957588646,
        "bert_precision": 0.8405869603157043,
        "bert_recall": 0.8925598859786987,
        "bert_f1": 0.8657941222190857,
        "meteor": 0.13333333333333333,
        "sbert_similarity": 0.5785777568817139
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of art does Caroline make?",
      "prediction": "Caroline makes paintings that express her trans experience and symbolize acceptance and self-love.",
      "reference": "abstract art",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8284295201301575,
        "bert_recall": 0.8711715936660767,
        "bert_f1": 0.849263072013855,
        "meteor": 0.0,
        "sbert_similarity": 0.3575077950954437
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie's daughter's birthday?",
      "prediction": "14 August, 2023",
      "reference": "13 August",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8588045835494995,
        "bert_recall": 0.9406881332397461,
        "bert_f1": 0.8978833556175232,
        "meteor": 0.0,
        "sbert_similarity": 0.7072815299034119
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline attend a pride parade in August?",
      "prediction": "last Friday, 11 August 2023",
      "reference": "The Friday before 14 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.5,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1384586850774637,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9479648470878601,
        "bert_recall": 0.9374210238456726,
        "bert_f1": 0.9426634907722473,
        "meteor": 0.3177966101694915,
        "sbert_similarity": 0.8606679439544678
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered an ally to the transgender community?",
      "prediction": "Yes, Melanie supports Caroline and encourages awareness and involvement in the LGBTQ community.",
      "reference": "Yes, she is supportive",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.13333333333333336,
        "bleu2": 0.09759000729485329,
        "bleu3": 0.043192289467872375,
        "bleu4": 0.027952555963587523,
        "bert_precision": 0.8731280565261841,
        "bert_recall": 0.910496711730957,
        "bert_f1": 0.8914209604263306,
        "meteor": 0.20408163265306123,
        "sbert_similarity": 0.5085012912750244
      }
    },
    {
      "sample_id": 0,
      "question": "Who supports Caroline when she has a negative experience?",
      "prediction": "my friends, family and mentors are my rocks \u2013 they motivate me and give me the strength to push on.",
      "reference": "Her mentors, family, and friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0909090909090909,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.22727272727272727,
        "bleu2": 0.10403129732205985,
        "bleu3": 0.03908304124158772,
        "bleu4": 0.02310128340501537,
        "bert_precision": 0.8752371072769165,
        "bert_recall": 0.9371011257171631,
        "bert_f1": 0.9051132202148438,
        "meteor": 0.07692307692307694,
        "sbert_similarity": 0.3961755037307739
      }
    },
    {
      "sample_id": 0,
      "question": "What types of pottery have Melanie and her kids made?",
      "prediction": "We all made our own pots.",
      "reference": "bowls, cup",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8648648858070374,
        "bert_recall": 0.8462418913841248,
        "bert_f1": 0.8554520606994629,
        "meteor": 0.0,
        "sbert_similarity": 0.28624391555786133
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline and Melanie go to a pride fesetival together?",
      "prediction": "last year at the Pride fest",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7912019491195679,
        "bert_recall": 0.8358184695243835,
        "bert_f1": 0.8128985166549683,
        "meteor": 0.0,
        "sbert_similarity": 0.20312747359275818
      }
    },
    {
      "sample_id": 0,
      "question": "What would Caroline's political leaning likely be?",
      "prediction": "supporting the LGBTQ community",
      "reference": "Liberal",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8052605390548706,
        "bert_recall": 0.8899629712104797,
        "bert_f1": 0.8454955816268921,
        "meteor": 0.0,
        "sbert_similarity": 0.31286975741386414
      }
    },
    {
      "sample_id": 0,
      "question": "What has Melanie painted?",
      "prediction": "a calming painting with blue streaks",
      "reference": "Horse, sunset, sunrise",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8331187963485718,
        "bert_recall": 0.8312579989433289,
        "bert_f1": 0.8321873545646667,
        "meteor": 0.0,
        "sbert_similarity": 0.32619598507881165
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's pets' names?",
      "prediction": "Luna and Oliver!",
      "reference": "Oliver, Luna, Bailey",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.38940039153570244,
        "bleu2": 0.10054274876162822,
        "bleu3": 0.07504064673338035,
        "bleu4": 0.07440995947486906,
        "bert_precision": 0.8934439420700073,
        "bert_recall": 0.8757719993591309,
        "bert_f1": 0.8845196962356567,
        "meteor": 0.0,
        "sbert_similarity": 0.7966151237487793
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline apply to adoption agencies?",
      "prediction": "23 August, 2023",
      "reference": "The week of 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9311424493789673,
        "bert_recall": 0.8844870924949646,
        "bert_f1": 0.9072153568267822,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8501458168029785
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline draw a self-portrait?",
      "prediction": "last week on 23 August, 2023",
      "reference": "The week before 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.4000000000000001,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.3086066999241838,
        "bleu3": 0.12657485247324374,
        "bleu4": 0.08307018474412793,
        "bert_precision": 0.9407693147659302,
        "bert_recall": 0.949950635433197,
        "bert_f1": 0.94533771276474,
        "meteor": 0.25,
        "sbert_similarity": 0.8732799291610718
      }
    },
    {
      "sample_id": 0,
      "question": "What subject have Caroline and Melanie both painted?",
      "prediction": "painting",
      "reference": "Sunsets",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8751158714294434,
        "bert_recall": 0.8036508560180664,
        "bert_f1": 0.8378621935844421,
        "meteor": 0.0,
        "sbert_similarity": 0.28051096200942993
      }
    },
    {
      "sample_id": 0,
      "question": "What symbols are important to Caroline?",
      "prediction": "a necklace symbolizing love, faith and strength",
      "reference": "Rainbow flag, transgender symbol",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8601879477500916,
        "bert_recall": 0.8608937859535217,
        "bert_f1": 0.8605406880378723,
        "meteor": 0.11627906976744187,
        "sbert_similarity": 0.36698275804519653
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline encounter people on a hike and have a negative experience?",
      "prediction": "No date mentioned.",
      "reference": "The week before 25 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8768361210823059,
        "bert_recall": 0.7974886894226074,
        "bert_f1": 0.8352822661399841,
        "meteor": 0.0,
        "sbert_similarity": 0.4493759274482727
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie make a plate in pottery class?",
      "prediction": "yesterday, 25 August 2023",
      "reference": "24 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9253772497177124,
        "bert_recall": 0.9674327373504639,
        "bert_f1": 0.9459378123283386,
        "meteor": 0.6048387096774195,
        "sbert_similarity": 0.9063760042190552
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline be considered religious?",
      "prediction": "Caroline shares a necklace with a cross and a heart.",
      "reference": "Somewhat, but not extremely religious",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8652608394622803,
        "bert_recall": 0.8291574716567993,
        "bert_f1": 0.8468245267868042,
        "meteor": 0.0,
        "sbert_similarity": 0.08047310262918472
      }
    },
    {
      "sample_id": 0,
      "question": "What instruments does Melanie play?",
      "prediction": "clarinet",
      "reference": "clarinet and violin",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9644248485565186,
        "bert_recall": 0.8998497724533081,
        "bert_f1": 0.9310189485549927,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.8449621200561523
      }
    },
    {
      "sample_id": 0,
      "question": "What musical artists/bands has Melanie seen?",
      "prediction": "the band",
      "reference": "Summer Sounds, Matt Patterson",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8766264915466309,
        "bert_recall": 0.8478443622589111,
        "bert_f1": 0.8619952201843262,
        "meteor": 0.0,
        "sbert_similarity": 0.23492103815078735
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the park?",
      "prediction": "on 13 September, 2023",
      "reference": "27 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.9223561882972717,
        "bert_recall": 0.9629870057106018,
        "bert_f1": 0.9422338008880615,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.8180784583091736
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline's youth center putting on a talent show?",
      "prediction": "next month",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8689342737197876,
        "bert_recall": 0.8372640013694763,
        "bert_f1": 0.8528052568435669,
        "meteor": 0.0,
        "sbert_similarity": 0.536835789680481
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie likely enjoy the song \"The Four Seasons\" by Vivaldi?",
      "prediction": "I'm a fan of both classical like Bach and Mozart",
      "reference": "Yes; it's classical music",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8566728830337524,
        "bert_recall": 0.8774056434631348,
        "bert_f1": 0.8669153451919556,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.5529870986938477
      }
    },
    {
      "sample_id": 0,
      "question": "What are some changes Caroline has faced during her transition journey?",
      "prediction": "Realizing I can be me without fear and having the courage to transition was the best part.",
      "reference": "Changes to her body, losing unsupportive friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913043,
        "rouge1_f": 0.08333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333333,
        "bleu1": 0.05555555555555554,
        "bleu2": 0.018077538151554672,
        "bleu3": 0.013254163846385744,
        "bleu4": 0.010802314890908065,
        "bert_precision": 0.8493291139602661,
        "bert_recall": 0.846173107624054,
        "bert_f1": 0.8477481007575989,
        "meteor": 0.0625,
        "sbert_similarity": 0.17026735842227936
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do with her family on hikes?",
      "prediction": "explored nature, roasted marshmallows around the campfire and even went on a hike",
      "reference": "Roast marshmallows, tell stories",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.13333333333333333,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.033149677206589796,
        "bleu3": 0.021746357799665227,
        "bleu4": 0.0169861974906263,
        "bert_precision": 0.8712518215179443,
        "bert_recall": 0.8845518231391907,
        "bert_f1": 0.8778514862060547,
        "meteor": 0.10204081632653061,
        "sbert_similarity": 0.7075150609016418
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go biking with friends?",
      "prediction": "last weekend on 13 September, 2023",
      "reference": "The weekend before 13 September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.4000000000000001,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.3086066999241838,
        "bleu3": 0.12657485247324374,
        "bleu4": 0.08307018474412793,
        "bert_precision": 0.943183422088623,
        "bert_recall": 0.9521592855453491,
        "bert_f1": 0.947650134563446,
        "meteor": 0.25,
        "sbert_similarity": 0.9040927290916443
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been practicing art?",
      "prediction": "Seven years now.",
      "reference": "Since 2016",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8721542954444885,
        "bert_recall": 0.8533459305763245,
        "bert_f1": 0.8626476526260376,
        "meteor": 0.0,
        "sbert_similarity": 0.49278587102890015
      }
    },
    {
      "sample_id": 0,
      "question": "What personality traits might Melanie say Caroline has?",
      "prediction": "passion, hard work, authenticity, creativity, support",
      "reference": "Thoughtful, authentic, driven",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.2,
        "bleu2": 0.04714045207910317,
        "bleu3": 0.03136318607896979,
        "bleu4": 0.025098621243978974,
        "bert_precision": 0.8585007786750793,
        "bert_recall": 0.8519311547279358,
        "bert_f1": 0.8552033305168152,
        "meteor": 0.0,
        "sbert_similarity": 0.4723640978336334
      }
    },
    {
      "sample_id": 0,
      "question": "What transgender-specific events has Caroline attended?",
      "prediction": "a transgender conference, a transgender poetry reading, and an LGBTQ+ pride parade",
      "reference": "Poetry reading, conference",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4615384615384615,
        "rouge1_f": 0.4,
        "rouge2_f": 0.15384615384615385,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.20965696734438366,
        "bleu3": 0.15706026521556773,
        "bleu4": 0.07596458453655348,
        "bert_precision": 0.8610671758651733,
        "bert_recall": 0.9258628487586975,
        "bert_f1": 0.8922902345657349,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.5694990754127502
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Melanie read from Caroline's suggestion?",
      "prediction": "that book you recommended a while ago",
      "reference": "\"Becoming Nicole\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8276762962341309,
        "bert_recall": 0.8288826942443848,
        "bert_f1": 0.8282790184020996,
        "meteor": 0.0,
        "sbert_similarity": 0.11817925423383713
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's friend adopt a child?",
      "prediction": "last year",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694186806678772,
        "bert_recall": 0.9111441373825073,
        "bert_f1": 0.8897925019264221,
        "meteor": 0.0,
        "sbert_similarity": 0.3672372102737427
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie get hurt?",
      "prediction": "Not mentioned.",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8867291212081909,
        "bert_recall": 0.8151055574417114,
        "bert_f1": 0.8494101762771606,
        "meteor": 0.0,
        "sbert_similarity": 0.1948297917842865
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's family go on a roadtrip?",
      "prediction": "Not mentioned.",
      "reference": "The weekend before 20 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8874698877334595,
        "bert_recall": 0.795289158821106,
        "bert_f1": 0.8388547301292419,
        "meteor": 0.0,
        "sbert_similarity": 0.21449434757232666
      }
    },
    {
      "sample_id": 0,
      "question": "How many children does Melanie have?",
      "prediction": "the whole gang",
      "reference": 3,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8257356286048889,
        "bert_recall": 0.9048750996589661,
        "bert_f1": 0.8634958863258362,
        "meteor": 0.0,
        "sbert_similarity": 0.24798011779785156
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go on a hike after the roadtrip?",
      "prediction": "yesterday",
      "reference": "19 October 2023",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8904598951339722,
        "bert_recall": 0.7936490774154663,
        "bert_f1": 0.8392719030380249,
        "meteor": 0.0,
        "sbert_similarity": 0.3886603116989136
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie go on another roadtrip soon?",
      "prediction": "What's been up since we last talked?",
      "reference": "Likely no; since this one went badly",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8225690126419067,
        "bert_recall": 0.820659875869751,
        "bert_f1": 0.8216133117675781,
        "meteor": 0.07142857142857142,
        "sbert_similarity": 0.14572277665138245
      }
    },
    {
      "sample_id": 0,
      "question": "What items has Melanie bought?",
      "prediction": "pottery",
      "reference": "Figurines, shoes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8555772304534912,
        "bert_recall": 0.7900184988975525,
        "bert_f1": 0.8214919567108154,
        "meteor": 0.0,
        "sbert_similarity": 0.3700336515903473
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline pass the adoption interview?",
      "prediction": "Last Friday, 20 October, 2023.",
      "reference": "The Friday before 22 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.375,
        "bleu2": 0.07319250547114,
        "bleu3": 0.04610606439635425,
        "bleu4": 0.03655552228545124,
        "bert_precision": 0.9396185874938965,
        "bert_recall": 0.9308847188949585,
        "bert_f1": 0.9352312684059143,
        "meteor": 0.0,
        "sbert_similarity": 0.8494569063186646
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie buy the figurines?",
      "prediction": "yesterday, on 21 October, 2023",
      "reference": "21 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.9083651900291443,
        "bert_recall": 0.9607000946998596,
        "bert_f1": 0.9337999820709229,
        "meteor": 0.3125,
        "sbert_similarity": 0.8718471527099609
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline want to move back to her home country soon?",
      "prediction": "The context does not provide any information about Caroline wanting to move back to her home country.",
      "reference": "No; she's in the process of adopting children.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08333333333333333,
        "rouge1_f": 0.07692307692307691,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07692307692307691,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.02556549962824567,
        "bleu3": 0.016660661223010558,
        "bleu4": 0.01284618972676772,
        "bert_precision": 0.8527673482894897,
        "bert_recall": 0.8810056447982788,
        "bert_f1": 0.866656482219696,
        "meteor": 0.05617977528089888,
        "sbert_similarity": 0.28098997473716736
      }
    },
    {
      "sample_id": 0,
      "question": "What did the charity race raise awareness for?",
      "prediction": "mental health",
      "reference": "mental health",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie realize after the charity race?",
      "prediction": "Really made me think about taking care of our minds.",
      "reference": "self-care is important",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8738052845001221,
        "bert_recall": 0.8957988023757935,
        "bert_f1": 0.8846653699874878,
        "meteor": 0.0,
        "sbert_similarity": 0.3609926998615265
      }
    },
    {
      "sample_id": 0,
      "question": "How does Melanie prioritize self-care?",
      "prediction": "I'm carving out some me-time each day - running, reading, or playing my violin - which refreshes me and helps me stay present for my fam!",
      "reference": "by carving out some me-time each day for activities like running, reading, or playing the violin",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.6046511627906976,
        "rouge2_f": 0.43902439024390244,
        "rougeL_f": 0.558139534883721,
        "bleu1": 0.4666666666666667,
        "bleu2": 0.40114777854287703,
        "bleu3": 0.361941551554555,
        "bleu4": 0.3179303998422513,
        "bert_precision": 0.902489423751831,
        "bert_recall": 0.9424307346343994,
        "bert_f1": 0.9220277667045593,
        "meteor": 0.6928104575163399,
        "sbert_similarity": 0.6163123250007629
      }
    },
    {
      "sample_id": 0,
      "question": "What are Caroline's plans for the summer?",
      "prediction": "Wanna do a family outing this summer?",
      "reference": "researching adoption agencies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8271617293357849,
        "bert_recall": 0.8431306481361389,
        "bert_f1": 0.8350698351860046,
        "meteor": 0.0,
        "sbert_similarity": 0.1108405813574791
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Caroline is considering support?",
      "prediction": "They help LGBTQ+ folks with adoption.",
      "reference": "LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8555476665496826,
        "bert_recall": 0.8629321455955505,
        "bert_f1": 0.8592240214347839,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6032156944274902
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose the adoption agency?",
      "prediction": "'cause they help LGBTQ+ folks with adoption. Their inclusivity and support really spoke to me.",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4166666666666667,
        "rouge1_f": 0.4166666666666667,
        "rouge2_f": 0.2727272727272727,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.29411764705882354,
        "bleu2": 0.23483410915693106,
        "bleu3": 0.1976660848079421,
        "bleu4": 0.1513851459876605,
        "bert_precision": 0.8940016031265259,
        "bert_recall": 0.9144972562789917,
        "bert_f1": 0.9041332602500916,
        "meteor": 0.5041666666666667,
        "sbert_similarity": 0.736127495765686
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline excited about in the adoption process?",
      "prediction": "I'm stoked to start this new chapter. It's been a dream to adopt and provide a safe, loving home for kids who need it.",
      "reference": "creating a family for kids who need one",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.29411764705882354,
        "rouge2_f": 0.1875,
        "rougeL_f": 0.29411764705882354,
        "bleu1": 0.1724137931034483,
        "bleu2": 0.13591507055489058,
        "bleu3": 0.1134870957807143,
        "bleu4": 0.08517399705356635,
        "bert_precision": 0.8565651774406433,
        "bert_recall": 0.8886820077896118,
        "bert_f1": 0.8723281025886536,
        "meteor": 0.5041666666666667,
        "sbert_similarity": 0.45787984132766724
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie think about Caroline's decision to adopt?",
      "prediction": "You're doing something amazing! Creating a family for those kids is so lovely. You'll be an awesome mom!",
      "reference": "she thinks Caroline is doing something amazing and will be an awesome mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5161290322580646,
        "rouge1_f": 0.4848484848484849,
        "rouge2_f": 0.3225806451612903,
        "rougeL_f": 0.4242424242424242,
        "bleu1": 0.3478260869565218,
        "bleu2": 0.28116077855776667,
        "bleu3": 0.22773394707102293,
        "bleu4": 0.15415064977510756,
        "bert_precision": 0.8964871168136597,
        "bert_recall": 0.9144225120544434,
        "bert_f1": 0.9053660035133362,
        "meteor": 0.4166666666666667,
        "sbert_similarity": 0.49288731813430786
      }
    },
    {
      "sample_id": 0,
      "question": "How long have Mel and her husband been married?",
      "prediction": "5 years already!",
      "reference": "Mel and her husband have been married for 5 years.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.08688697172522257,
        "bleu2": 0.07094291534080824,
        "bleu3": 0.035797677029744235,
        "bleu4": 0.029524961852386813,
        "bert_precision": 0.8518999218940735,
        "bert_recall": 0.8372283577919006,
        "bert_f1": 0.8445004224777222,
        "meteor": 0.053763440860215055,
        "sbert_similarity": 0.28969717025756836
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's necklace symbolize?",
      "prediction": "love, faith and strength",
      "reference": "love, faith, and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8187307530779819,
        "bleu2": 0.7090416310250969,
        "bleu3": 0.5181569246973543,
        "bleu4": 0.2737591267534727,
        "bert_precision": 0.9906566739082336,
        "bert_recall": 0.9857161045074463,
        "bert_f1": 0.9881802201271057,
        "meteor": 0.6388888888888888,
        "sbert_similarity": 0.9917946457862854
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Caroline's grandma from?",
      "prediction": "The context does not provide information about Caroline's grandma's country.",
      "reference": "Sweden",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8028200268745422,
        "bert_recall": 0.8221871256828308,
        "bert_f1": 0.8123881220817566,
        "meteor": 0.0,
        "sbert_similarity": 0.1072012335062027
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Caroline?",
      "prediction": "a necklace",
      "reference": "necklace",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8547327518463135,
        "bert_recall": 0.9164279103279114,
        "bert_f1": 0.8845058083534241,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.9594278335571289
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family do while camping?",
      "prediction": "explored nature, roasted marshmallows around the campfire and even went on a hike.",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8181818181818181,
        "rouge1_f": 0.8181818181818181,
        "rouge2_f": 0.6,
        "rougeL_f": 0.8181818181818181,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.5077018911399525,
        "bleu4": 0.42311785416105785,
        "bert_precision": 0.9487875699996948,
        "bert_recall": 0.9758627414703369,
        "bert_f1": 0.9621347188949585,
        "meteor": 0.7153712548849327,
        "sbert_similarity": 0.9060356020927429
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Caroline interested in pursuing?",
      "prediction": "I'm thinking of working with trans people, helping them accept themselves and supporting their mental health.",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.896551724137931,
        "rouge1_f": 0.8666666666666666,
        "rouge2_f": 0.8571428571428571,
        "rougeL_f": 0.8666666666666666,
        "bleu1": 0.7368421052631579,
        "bleu2": 0.7294955398698874,
        "bleu3": 0.7238967222455629,
        "bleu4": 0.7128736748247595,
        "bert_precision": 0.9448204040527344,
        "bert_recall": 0.9558389186859131,
        "bert_f1": 0.9502977728843689,
        "meteor": 0.9019945697577276,
        "sbert_similarity": 0.9358103275299072
      }
    },
    {
      "sample_id": 0,
      "question": "What workshop did Caroline attend recently?",
      "prediction": "LGBTQ support group",
      "reference": "LGBTQ+ counseling workshop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8966649174690247,
        "bert_recall": 0.8552901744842529,
        "bert_f1": 0.8754890561103821,
        "meteor": 0.0,
        "sbert_similarity": 0.7010542750358582
      }
    },
    {
      "sample_id": 0,
      "question": "What was discussed in the LGBTQ+ counseling workshop?",
      "prediction": "different therapeutic methods and how to best work with trans people",
      "reference": "therapeutic methods and how to best work with trans people",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9523809523809523,
        "rouge1_f": 0.9523809523809523,
        "rouge2_f": 0.9473684210526316,
        "rougeL_f": 0.9523809523809523,
        "bleu1": 0.9090909090909091,
        "bleu2": 0.9045340337332909,
        "bleu3": 0.9002437389487007,
        "bleu4": 0.8931539818068694,
        "bert_precision": 0.9808754920959473,
        "bert_recall": 0.9933833479881287,
        "bert_f1": 0.9870897531509399,
        "meteor": 0.9896039603960397,
        "sbert_similarity": 0.985811710357666
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Caroline to pursue counseling?",
      "prediction": "My own journey and the support I got made a huge difference.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4166666666666667,
        "rouge1_f": 0.3846153846153846,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.3846153846153846,
        "bleu1": 0.3297707381386313,
        "bleu2": 0.30699984978524775,
        "bleu3": 0.2835206353227474,
        "bleu4": 0.24794277351227223,
        "bert_precision": 0.933977484703064,
        "bert_recall": 0.9229705333709717,
        "bert_f1": 0.9284414052963257,
        "meteor": 0.32318840579710145,
        "sbert_similarity": 0.4710140526294708
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Caroline want to create for people?",
      "prediction": "a supportive community that fosters inspiration and helps others",
      "reference": "a safe and inviting place for people to grow",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.05270462766947299,
        "bleu3": 0.03528075103412339,
        "bleu4": 0.028517539529041507,
        "bert_precision": 0.8857610821723938,
        "bert_recall": 0.8930854797363281,
        "bert_f1": 0.8894082903862,
        "meteor": 0.1111111111111111,
        "sbert_similarity": 0.41698503494262695
      }
    },
    {
      "sample_id": 0,
      "question": "Did Melanie make the black and white bowl in the photo?",
      "prediction": "Yeah, I made this bowl in my class.",
      "reference": "Yes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.829147219657898,
        "bert_recall": 0.8540465235710144,
        "bert_f1": 0.8414127230644226,
        "meteor": 0.0,
        "sbert_similarity": 0.13204294443130493
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of books does Caroline have in her library?",
      "prediction": "lots of kids' books- classics, stories from different cultures, educational books",
      "reference": "kids' books - classics, stories from different cultures, educational books",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7999999999999999,
        "rouge1_f": 0.9,
        "rouge2_f": 0.888888888888889,
        "rougeL_f": 0.9,
        "bleu1": 0.7857142857142857,
        "bleu2": 0.7375337578416624,
        "bleu3": 0.6846815074766505,
        "bleu4": 0.6450001140844255,
        "bert_precision": 0.9514201283454895,
        "bert_recall": 0.9638512134552002,
        "bert_f1": 0.9575954079627991,
        "meteor": 0.785891089108911,
        "sbert_similarity": 0.9294910430908203
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's favorite book from her childhood?",
      "prediction": "I loved reading \"Charlotte's Web\" as a kid.",
      "reference": "\"Charlotte's Web\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.4166666666666667,
        "bleu2": 0.3892494720807615,
        "bleu3": 0.3605790708802296,
        "bleu4": 0.31702331385234306,
        "bert_precision": 0.903247594833374,
        "bert_recall": 0.9491208791732788,
        "bert_f1": 0.9256162047386169,
        "meteor": 0.7211538461538463,
        "sbert_similarity": 0.729093611240387
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Caroline recommend to Melanie?",
      "prediction": "the book you recommended a while ago",
      "reference": "\"Becoming Nicole\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8305058479309082,
        "bert_recall": 0.8312391042709351,
        "bert_f1": 0.8308722972869873,
        "meteor": 0.0,
        "sbert_similarity": 0.10312724858522415
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline take away from the book \"Becoming Nicole\"?",
      "prediction": "It made me feel connected and gave me a lot of hope for my own path.",
      "reference": "Lessons on self-acceptance and finding support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809522,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8656476140022278,
        "bert_recall": 0.8624588847160339,
        "bert_f1": 0.8640503287315369,
        "meteor": 0.14285714285714288,
        "sbert_similarity": 0.309200644493103
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Melanie got used for?",
      "prediction": "They really liven up the house!",
      "reference": "Running",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8222196102142334,
        "bert_recall": 0.8554714918136597,
        "bert_f1": 0.8385159969329834,
        "meteor": 0.0,
        "sbert_similarity": 0.08845742791891098
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's reason for getting into running?",
      "prediction": "a great way to destress and clear my mind.",
      "reference": "To de-stress and clear her mind",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5333333333333333,
        "rouge1_f": 0.5,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.5,
        "bleu1": 0.4,
        "bleu2": 0.21081851067789192,
        "bleu3": 0.08428689847931439,
        "bleu4": 0.05307712171072445,
        "bert_precision": 0.9062039852142334,
        "bert_recall": 0.9304516315460205,
        "bert_f1": 0.918167769908905,
        "meteor": 0.40564373897707234,
        "sbert_similarity": 0.5757232308387756
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie say running has been great for?",
      "prediction": "a great way to destress and clear my mind",
      "reference": "Her mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8360733985900879,
        "bert_recall": 0.8746476173400879,
        "bert_f1": 0.8549256324768066,
        "meteor": 0.0,
        "sbert_similarity": 0.34080180525779724
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids make during the pottery workshop?",
      "prediction": "we all made our own pots",
      "reference": "pots",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.810404360294342,
        "bert_recall": 0.8825975656509399,
        "bert_f1": 0.8449617028236389,
        "meteor": 0.33333333333333337,
        "sbert_similarity": 0.6579083800315857
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of pot did Mel and her kids make with clay?",
      "prediction": "We all made our own pots.",
      "reference": "a cup with a dog face on it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8792178630828857,
        "bert_recall": 0.851855993270874,
        "bert_f1": 0.8653206825256348,
        "meteor": 0.0,
        "sbert_similarity": 0.09348646551370621
      }
    },
    {
      "sample_id": 0,
      "question": "What creative project do Mel and her kids do together besides pottery?",
      "prediction": "making clay sculptures",
      "reference": "painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.818657398223877,
        "bert_recall": 0.9070415496826172,
        "bert_f1": 0.8605861067771912,
        "meteor": 0.0,
        "sbert_similarity": 0.41896122694015503
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids paint in their latest project in July 2023?",
      "prediction": "another painting like our last one",
      "reference": "a sunset with a palm tree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8489360213279724,
        "bert_recall": 0.8746339082717896,
        "bert_f1": 0.8615934252738953,
        "meteor": 0.0,
        "sbert_similarity": 0.1922014355659485
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline see at the council meeting for adoption?",
      "prediction": "It was inspiring and emotional - so many people wanted to create loving homes for children in need.",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6896551724137931,
        "rouge1_f": 0.7857142857142858,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.7857142857142858,
        "bleu1": 0.5263157894736842,
        "bleu2": 0.4836508334066744,
        "bleu3": 0.4390690748074934,
        "bleu4": 0.4007781700028195,
        "bert_precision": 0.9035575985908508,
        "bert_recall": 0.948448121547699,
        "bert_f1": 0.9254588484764099,
        "meteor": 0.8542735042735045,
        "sbert_similarity": 0.7657339572906494
      }
    },
    {
      "sample_id": 0,
      "question": "What do sunflowers represent according to Caroline?",
      "prediction": "Sunflowers mean warmth and happiness.",
      "reference": "warmth and happiness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.5,
        "bleu2": 0.447213595499958,
        "bleu3": 0.3721003696282278,
        "bleu4": 0.20205155046766235,
        "bert_precision": 0.847357451915741,
        "bert_recall": 0.9021207094192505,
        "bert_f1": 0.873881995677948,
        "meteor": 0.5859375,
        "sbert_similarity": 0.5556073784828186
      }
    },
    {
      "sample_id": 0,
      "question": "Why are flowers important to Melanie?",
      "prediction": "Flowers bring joy. They represent growth, beauty and remind us to appreciate the small moments.",
      "reference": "They remind her to appreciate the small moments and were a part of her wedding decor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5333333333333333,
        "rouge1_f": 0.5161290322580646,
        "rouge2_f": 0.2758620689655172,
        "rougeL_f": 0.45161290322580644,
        "bleu1": 0.4444444444444444,
        "bleu2": 0.3233808333817773,
        "bleu3": 0.27321330311118297,
        "bleu4": 0.22612164703205964,
        "bert_precision": 0.901397705078125,
        "bert_recall": 0.874315083026886,
        "bert_f1": 0.8876498937606812,
        "meteor": 0.39917853934026437,
        "sbert_similarity": 0.5228031873703003
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's painting for the art show?",
      "prediction": "What inspired it?",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8489900231361389,
        "bert_recall": 0.8399085998535156,
        "bert_f1": 0.8444249629974365,
        "meteor": 0.0,
        "sbert_similarity": 0.09910623729228973
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Melanie go to the beach with her kids?",
      "prediction": "usually only once or twice a year",
      "reference": "once or twice a year",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.7142857142857143,
        "bleu2": 0.6900655593423543,
        "bleu3": 0.6613898798842307,
        "bleu4": 0.6147881529512643,
        "bert_precision": 0.925447404384613,
        "bert_recall": 0.949626088142395,
        "bert_f1": 0.9373809099197388,
        "meteor": 0.9576923076923077,
        "sbert_similarity": 0.9151639938354492
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family see during their camping trip last year?",
      "prediction": "the view from the top was amazing",
      "reference": "Perseid meteor shower",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8360108137130737,
        "bert_recall": 0.7674478888511658,
        "bert_f1": 0.8002634644508362,
        "meteor": 0.0,
        "sbert_similarity": 0.14809612929821014
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel while watching the meteor shower?",
      "prediction": "It felt like we were part of something huge and awe-inspiring.",
      "reference": "in awe of the universe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8544019460678101,
        "bert_recall": 0.8840770721435547,
        "bert_f1": 0.868986189365387,
        "meteor": 0.0892857142857143,
        "sbert_similarity": 0.4757840037345886
      }
    },
    {
      "sample_id": 0,
      "question": "Whose birthday did Melanie celebrate recently?",
      "prediction": "my daughter's birthday",
      "reference": "Melanie's daughter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.883353054523468,
        "bert_recall": 0.9120132327079773,
        "bert_f1": 0.8974543809890747,
        "meteor": 0.0,
        "sbert_similarity": 0.4458887577056885
      }
    },
    {
      "sample_id": 0,
      "question": "Who performed at the concert at Melanie's daughter's birthday?",
      "prediction": "The content does not specify who performed at the concert.",
      "reference": "Matt Patterson",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8363405466079712,
        "bert_recall": 0.8132212162017822,
        "bert_f1": 0.8246188759803772,
        "meteor": 0.0,
        "sbert_similarity": 0.0581410676240921
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose to use colors and patterns in her pottery project?",
      "prediction": "Pottery is a huge part of my life, not just a hobby - it helps me express my emotions.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.00913442366656447,
        "bert_precision": 0.8516536951065063,
        "bert_recall": 0.895057737827301,
        "bert_f1": 0.8728163838386536,
        "meteor": 0.0,
        "sbert_similarity": 0.06774991005659103
      }
    },
    {
      "sample_id": 0,
      "question": "What pet does Caroline have?",
      "prediction": "The content does not specify which pet Caroline has.",
      "reference": "guinea pig",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8217306137084961,
        "bert_recall": 0.7815132141113281,
        "bert_f1": 0.801117479801178,
        "meteor": 0.0,
        "sbert_similarity": 0.2717055082321167
      }
    },
    {
      "sample_id": 0,
      "question": "What pets does Melanie have?",
      "prediction": "a pup and a kitty",
      "reference": "Two cats and a dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4000000000000001,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4000000000000001,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9334430694580078,
        "bert_recall": 0.9434006214141846,
        "bert_f1": 0.9383954405784607,
        "meteor": 0.37500000000000006,
        "sbert_similarity": 0.7613353133201599
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oliver hide his bone once?",
      "prediction": "in my slipper",
      "reference": "In Melanie's slipper",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9412858486175537,
        "bert_recall": 0.9195078611373901,
        "bert_f1": 0.9302694201469421,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.6715267896652222
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Caroline used to do with her dad?",
      "prediction": "biking",
      "reference": "Horseback riding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8386521935462952,
        "bert_recall": 0.7486652135848999,
        "bert_f1": 0.7911079525947571,
        "meteor": 0.0,
        "sbert_similarity": 0.5904083847999573
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline make for a local church?",
      "prediction": "Caroline made a photo.",
      "reference": "a stained glass window",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8949602842330933,
        "bert_recall": 0.8488136529922485,
        "bert_f1": 0.8712763786315918,
        "meteor": 0.125,
        "sbert_similarity": 0.12657132744789124
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline find in her neighborhood during her walk?",
      "prediction": "Caroline found some pretty cool stuff.",
      "reference": "a rainbow sidewalk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8666286468505859,
        "bert_recall": 0.851978063583374,
        "bert_f1": 0.8592408895492554,
        "meteor": 0.0,
        "sbert_similarity": 0.13000726699829102
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Caroline to be courageous?",
      "prediction": "\"Brave\" by Sara Bareilles has a lot of significance for me. It's about being courageous and fighting for what's right.",
      "reference": "Brave by Sara Bareilles",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2608695652173913,
        "rouge1_f": 0.3076923076923077,
        "rouge2_f": 0.25,
        "rougeL_f": 0.3076923076923077,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.11094003924504585,
        "bleu3": 0.08208968293684403,
        "bleu4": 0.03864198149136354,
        "bert_precision": 0.8493939638137817,
        "bert_recall": 0.9284552335739136,
        "bert_f1": 0.8871666789054871,
        "meteor": 0.5257936507936508,
        "sbert_similarity": 0.6410200595855713
      }
    },
    {
      "sample_id": 0,
      "question": "Which  classical musicians does Melanie enjoy listening to?",
      "prediction": "Bach and Mozart",
      "reference": "Bach and Mozart",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Melanie a fan of in terms of modern music?",
      "prediction": "Ed Sheeran's \"Perfect\"",
      "reference": "Ed Sheeran",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.8977779746055603,
        "bert_recall": 0.9811892509460449,
        "bert_f1": 0.937632143497467,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7055439949035645
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been creating art?",
      "prediction": "Seven years now",
      "reference": "7 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9228770732879639,
        "bert_recall": 0.9635454416275024,
        "bert_f1": 0.9427728652954102,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.8659797310829163
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Melanie see at the caf\u00e9?",
      "prediction": "the sign was just a precaution",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705885,
        "rouge1_f": 0.23529411764705885,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352942,
        "bleu1": 0.14486606950235942,
        "bleu2": 0.03548479513233444,
        "bleu3": 0.024620565232033,
        "bleu4": 0.021099385422893017,
        "bert_precision": 0.8708813190460205,
        "bert_recall": 0.8462990522384644,
        "bert_f1": 0.8584142327308655,
        "meteor": 0.09523809523809525,
        "sbert_similarity": 0.5227034091949463
      }
    },
    {
      "sample_id": 0,
      "question": "What advice does Caroline give for getting started with adoption?",
      "prediction": "Do your research and find an adoption agency or lawyer. They'll help with the process and provide all the info.",
      "reference": "Do research, find an adoption agency or lawyer, gather necessary documents, and prepare emotionally.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5625000000000001,
        "rouge1_f": 0.5142857142857143,
        "rouge2_f": 0.30303030303030304,
        "rougeL_f": 0.5142857142857143,
        "bleu1": 0.43478260869565216,
        "bleu2": 0.3143473067309658,
        "bleu3": 0.2695490842678901,
        "bleu4": 0.23050898626566632,
        "bert_precision": 0.9083938598632812,
        "bert_recall": 0.9181722402572632,
        "bert_f1": 0.9132568836212158,
        "meteor": 0.46058149287112105,
        "sbert_similarity": 0.8086539506912231
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Melanie face in October 2023?",
      "prediction": "last month I got hurt and had to take a break from pottery",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8181818181818182,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.7692307692307693,
        "bleu2": 0.75955452531275,
        "bleu3": 0.7508082226694607,
        "bleu4": 0.7361703354503866,
        "bert_precision": 0.9596830606460571,
        "bert_recall": 0.9773361682891846,
        "bert_f1": 0.9684291481971741,
        "meteor": 0.8030202821869489,
        "sbert_similarity": 0.6912587881088257
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to keep herself busy during her pottery break?",
      "prediction": "Pottery is a huge part of my life, not just a hobby - it helps me express my emotions.",
      "reference": "Read a book and paint.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0909090909090909,
        "rouge1_f": 0.08695652173913045,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913045,
        "bleu1": 0.09523809523809525,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.0141800886003256,
        "bleu4": 0.010862721615727716,
        "bert_precision": 0.8367177248001099,
        "bert_recall": 0.8631895184516907,
        "bert_f1": 0.8497474789619446,
        "meteor": 0.078125,
        "sbert_similarity": 0.377413809299469
      }
    },
    {
      "sample_id": 0,
      "question": "What painting did Melanie show to Caroline on October 13, 2023?",
      "prediction": "this painting",
      "reference": "A painting inspired by sunsets with a pink sky.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.00915781944436709,
        "bleu2": 0.004095501360654834,
        "bleu3": 0.003187734728290661,
        "bleu4": 0.002738826828955665,
        "bert_precision": 0.9435615539550781,
        "bert_recall": 0.8413667678833008,
        "bert_f1": 0.8895386457443237,
        "meteor": 0.06024096385542168,
        "sbert_similarity": 0.5815021395683289
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of painting did Caroline share with Melanie on October 13, 2023?",
      "prediction": "a painting to show my path as a trans woman",
      "reference": "An abstract painting with blue streaks on a wall.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2105263157894737,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2105263157894737,
        "bleu1": 0.2,
        "bleu2": 0.04714045207910317,
        "bleu3": 0.03136318607896979,
        "bleu4": 0.025098621243978974,
        "bert_precision": 0.8753288388252258,
        "bert_recall": 0.8789288997650146,
        "bert_f1": 0.8771252632141113,
        "meteor": 0.10989010989010987,
        "sbert_similarity": 0.41244298219680786
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Caroline attended about?",
      "prediction": "It was a transgender poetry reading where transgender people shared their stories through poetry.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9565217391304348,
        "rouge1_f": 0.923076923076923,
        "rouge2_f": 0.9166666666666666,
        "rougeL_f": 0.923076923076923,
        "bleu1": 0.8666666666666667,
        "bleu2": 0.8251983888449983,
        "bleu3": 0.8078434700407627,
        "bleu4": 0.7916963878457504,
        "bert_precision": 0.9864805340766907,
        "bert_recall": 0.9956105947494507,
        "bert_f1": 0.9910245537757874,
        "meteor": 0.9013006367700853,
        "sbert_similarity": 0.9938864707946777
      }
    },
    {
      "sample_id": 0,
      "question": "What did the posters at the poetry reading say?",
      "prediction": "the posters were amazing, so much pride and strength!",
      "reference": "\"Trans Lives Matter\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8240386247634888,
        "bert_recall": 0.8319059610366821,
        "bert_f1": 0.827953577041626,
        "meteor": 0.13888888888888892,
        "sbert_similarity": 0.0876564085483551
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's drawing symbolize for her?",
      "prediction": "It stands for freedom and being real. It's like a nudge to always stay true to myself and embrace my womanhood.",
      "reference": "Freedom and being true to herself.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.39999999999999997,
        "rouge1_f": 0.35714285714285715,
        "rouge2_f": 0.23076923076923073,
        "rougeL_f": 0.35714285714285715,
        "bleu1": 0.25,
        "bleu2": 0.18057877962865382,
        "bleu3": 0.11652001888986878,
        "bleu4": 0.05154337627145953,
        "bert_precision": 0.8669825196266174,
        "bert_recall": 0.9182741641998291,
        "bert_f1": 0.891891598701477,
        "meteor": 0.49600000000000005,
        "sbert_similarity": 0.4861554801464081
      }
    },
    {
      "sample_id": 0,
      "question": "How do Melanie and Caroline describe their journey through life together?",
      "prediction": "Life's about learning and exploring. Glad we can be on this trip together.",
      "reference": "An ongoing adventure of learning and growing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.19047619047619047,
        "rouge2_f": 0.10526315789473684,
        "rougeL_f": 0.19047619047619047,
        "bleu1": 0.1875,
        "bleu2": 0.11180339887498951,
        "bleu3": 0.04610606439635425,
        "bleu4": 0.02878787818101127,
        "bert_precision": 0.8740041255950928,
        "bert_recall": 0.9006232023239136,
        "bert_f1": 0.8871140480041504,
        "meteor": 0.24671052631578952,
        "sbert_similarity": 0.5193980932235718
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Melanie's son on their road trip?",
      "prediction": "My son got into an accident.",
      "reference": "He got into an accident",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727272,
        "rouge1_f": 0.7272727272727272,
        "rouge2_f": 0.6666666666666665,
        "rougeL_f": 0.7272727272727272,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.5345224838248488,
        "bleu3": 0.4888069165703076,
        "bleu4": 0.41113361690051975,
        "bert_precision": 0.9502152800559998,
        "bert_recall": 0.9591536521911621,
        "bert_f1": 0.9546635150909424,
        "meteor": 0.5773420479302832,
        "sbert_similarity": 0.717858612537384
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's son handle the accident?",
      "prediction": "Thank goodness your son's okay.",
      "reference": "He was scared but reassured by his family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8654096722602844,
        "bert_recall": 0.8480106592178345,
        "bert_f1": 0.8566218018531799,
        "meteor": 0.0,
        "sbert_similarity": 0.2824382781982422
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family after the accident?",
      "prediction": "I'm really lucky to have my family; they bring so much joy and love.",
      "reference": "They are important and mean the world to her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2608695652173913,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.03321055820775358,
        "bleu3": 0.020227031939554035,
        "bleu4": 0.015138514598766055,
        "bert_precision": 0.8712875247001648,
        "bert_recall": 0.8919955492019653,
        "bert_f1": 0.8815199732780457,
        "meteor": 0.15789473684210528,
        "sbert_similarity": 0.17042067646980286
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's children handle the accident?",
      "prediction": "The kids look so cute, Mel! I bet they bring lots of joy.",
      "reference": "They were scared but resilient",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8337079286575317,
        "bert_recall": 0.8453997373580933,
        "bert_f1": 0.8395131230354309,
        "meteor": 0.08620689655172414,
        "sbert_similarity": 0.19902172684669495
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel after the accident?",
      "prediction": "Melanie felt happy and alive.",
      "reference": "Grateful and thankful for her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8691820502281189,
        "bert_recall": 0.8410165905952454,
        "bert_f1": 0.8548673987388611,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.4090595841407776
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's reaction to her children enjoying the Grand Canyon?",
      "prediction": "they enjoyed the Grand Canyon a lot!",
      "reference": "She was happy and thankful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8625913858413696,
        "bert_recall": 0.8752244710922241,
        "bert_f1": 0.868861973285675,
        "meteor": 0.0,
        "sbert_similarity": 0.20662683248519897
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's family give her?",
      "prediction": "love and support",
      "reference": "Strength and motivation",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9224023818969727,
        "bert_recall": 0.9224023818969727,
        "bert_f1": 0.9224023818969727,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.3396154046058655
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family supporting her?",
      "prediction": "My fam's been awesome - they helped out and showed lots of love and support.",
      "reference": "She appreciated them a lot",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8452944755554199,
        "bert_recall": 0.886775553226471,
        "bert_f1": 0.8655382990837097,
        "meteor": 0.08333333333333336,
        "sbert_similarity": 0.352303147315979
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie do after the road trip to relax?",
      "prediction": "the kids loved it and it was a nice way to relax after the road trip.",
      "reference": "Went on a nature walk or hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8501438498497009,
        "bert_recall": 0.8401069045066833,
        "bert_f1": 0.8450955748558044,
        "meteor": 0.06329113924050632,
        "sbert_similarity": 0.3132382929325104
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline realize after her charity race?",
      "prediction": "self-care is important",
      "reference": "self-care is important",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's plans for the summer with respect to adoption?",
      "prediction": "Not mentioned in the conversation",
      "reference": "researching adoption agencies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.870311439037323,
        "bert_recall": 0.8672590851783752,
        "bert_f1": 0.8687825798988342,
        "meteor": 0.0,
        "sbert_similarity": 0.010256906040012836
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Melanie is considering support?",
      "prediction": "LGBTQ+ individuals",
      "reference": "LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose the adoption agency?",
      "prediction": "Not mentioned in the conversation",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8659256100654602,
        "bert_recall": 0.806060254573822,
        "bert_f1": 0.8349211812019348,
        "meteor": 0.0,
        "sbert_similarity": 0.0991666167974472
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie excited about in her adoption process?",
      "prediction": "creating a family for kids who need one",
      "reference": "creating a family for kids who need one",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie's necklace symbolize?",
      "prediction": "Not mentioned in the conversation",
      "reference": "love, faith, and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8847730159759521,
        "bert_recall": 0.8560526371002197,
        "bert_f1": 0.8701758980751038,
        "meteor": 0.0,
        "sbert_similarity": 0.04011678695678711
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Melanie's grandma from?",
      "prediction": "Sweden",
      "reference": "Sweden",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Melanie?",
      "prediction": "Not mentioned in the conversation",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8458929657936096,
        "bert_recall": 0.8909751176834106,
        "bert_f1": 0.8678489327430725,
        "meteor": 0.0,
        "sbert_similarity": 0.17644497752189636
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandpa's gift to Caroline?",
      "prediction": "Not mentioned in the conversation",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8458929657936096,
        "bert_recall": 0.8909751176834106,
        "bert_f1": 0.8678489327430725,
        "meteor": 0.0,
        "sbert_similarity": 0.17644497752189636
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family do while camping?",
      "prediction": "explored nature, roasted marshmallows, and went on a hike",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Melanie interested in pursuing?",
      "prediction": "Not mentioned in the conversation",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8718171715736389,
        "bert_recall": 0.8251792192459106,
        "bert_f1": 0.8478572964668274,
        "meteor": 0.0,
        "sbert_similarity": 0.05818865820765495
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling workshop did Melanie attend recently?",
      "prediction": "LGBTQ+ counseling workshop",
      "reference": "LGBTQ+ counseling workshop",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Melanie to pursue counseling?",
      "prediction": "Not mentioned in the conversation.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.10526315789473682,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473682,
        "bleu1": 0.037188360024738305,
        "bleu2": 0.012882425802602028,
        "bleu3": 0.01005607412506085,
        "bleu4": 0.009109250643152446,
        "bert_precision": 0.8842162489891052,
        "bert_recall": 0.8450900912284851,
        "bert_f1": 0.8642105460166931,
        "meteor": 0.038167938931297704,
        "sbert_similarity": 0.007081456482410431
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Melanie want to create for people?",
      "prediction": "a safe and inviting place for people to grow",
      "reference": "a safe and inviting place for people to grow",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998211860657,
        "bert_recall": 0.9999998211860657,
        "bert_f1": 0.9999998211860657,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Did Caroline make the black and white bowl in the photo?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.848052978515625,
        "bert_recall": 0.8904046416282654,
        "bert_f1": 0.8687129020690918,
        "meteor": 0.0,
        "sbert_similarity": 0.11724156886339188
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Caroline got used for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Running",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8501412272453308,
        "bert_recall": 0.8962051272392273,
        "bert_f1": 0.8725656270980835,
        "meteor": 0.0,
        "sbert_similarity": 0.165440633893013
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's reason for getting into running?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To de-stress and clear her mind",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8849431872367859,
        "bert_recall": 0.8652690649032593,
        "bert_f1": 0.8749955892562866,
        "meteor": 0.0,
        "sbert_similarity": 0.15259520709514618
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline say running has been great for?",
      "prediction": "Her mental health",
      "reference": "Her mental health",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie see at the council meeting for adoption?",
      "prediction": "many people wanting to create loving homes for children in need",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999997615814209,
        "bert_recall": 0.9999997615814209,
        "bert_f1": 0.9999997615814209,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Melanie's painting for the art show?",
      "prediction": "Not mentioned in the conversation",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672127723693848,
        "bert_recall": 0.8251693844795227,
        "bert_f1": 0.8456689119338989,
        "meteor": 0.0,
        "sbert_similarity": -0.0016055349260568619
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's sculpture for the art show?",
      "prediction": "visiting an LGBTQ center and wanting to capture unity and strength",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Caroline go to the beach with her kids?",
      "prediction": "Not mentioned in the conversation",
      "reference": "once or twice a year",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8639418482780457,
        "bert_recall": 0.8075608015060425,
        "bert_f1": 0.8348004817962646,
        "meteor": 0.0,
        "sbert_similarity": 0.05305669829249382
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family see during their camping trip last year?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Perseid meteor shower",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8497328758239746,
        "bert_recall": 0.7694870829582214,
        "bert_f1": 0.8076215386390686,
        "meteor": 0.0,
        "sbert_similarity": 0.033341605216264725
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel while watching the meteor shower?",
      "prediction": "in awe of the universe",
      "reference": "in awe of the universe",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose to use colors and patterns in her pottery project?",
      "prediction": "She wanted to catch the eye and make people smile.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998211860657,
        "bert_recall": 0.9999998211860657,
        "bert_f1": 0.9999998211860657,
        "meteor": 0.9995,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Is Oscar Melanie's pet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.848052978515625,
        "bert_recall": 0.8904046416282654,
        "bert_f1": 0.8687129020690918,
        "meteor": 0.0,
        "sbert_similarity": 0.11724156886339188
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oscar hide his bone once?",
      "prediction": "In Melanie's slipper",
      "reference": "In Melanie's slipper",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Melanie used to do with her dad?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Horseback riding",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8533902168273926,
        "bert_recall": 0.7734711766242981,
        "bert_f1": 0.8114677667617798,
        "meteor": 0.0,
        "sbert_similarity": 0.05627928674221039
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie make for a local church?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a stained glass window",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8690841794013977,
        "bert_recall": 0.836325466632843,
        "bert_f1": 0.8523901700973511,
        "meteor": 0.0,
        "sbert_similarity": 0.036170102655887604
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie find in her neighborhood during her walk?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a rainbow sidewalk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8700210452079773,
        "bert_recall": 0.856080949306488,
        "bert_f1": 0.8629946708679199,
        "meteor": 0.0,
        "sbert_similarity": -0.048551272600889206
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Melanie to be courageous?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Brave by Sara Bareilles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8708803057670593,
        "bert_recall": 0.8365832567214966,
        "bert_f1": 0.8533873558044434,
        "meteor": 0.0,
        "sbert_similarity": -0.004309710115194321
      }
    },
    {
      "sample_id": 0,
      "question": "What type of instrument does Caroline play?",
      "prediction": "Caroline plays the guitar.",
      "reference": "clarinet and violin",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8864635229110718,
        "bert_recall": 0.8374868631362915,
        "bert_f1": 0.8612794876098633,
        "meteor": 0.0,
        "sbert_similarity": 0.28861886262893677
      }
    },
    {
      "sample_id": 0,
      "question": "Which classical musicians does Caroline enjoy listening to?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Bach and Mozart",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8735334873199463,
        "bert_recall": 0.8279000520706177,
        "bert_f1": 0.850104808807373,
        "meteor": 0.0,
        "sbert_similarity": 0.10292421281337738
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Caroline a fan of in terms of modern music?",
      "prediction": "Ed Sheeran",
      "reference": "Ed Sheeran",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999997615814209,
        "bert_recall": 0.9999997615814209,
        "bert_f1": 0.9999997615814209,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Caroline see at the caf\u00e9?",
      "prediction": "A sign stating that someone is not being able to leave",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Caroline face recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8869463801383972,
        "bert_recall": 0.8621917963027954,
        "bert_f1": 0.8743939399719238,
        "meteor": 0.0,
        "sbert_similarity": 0.0377417653799057
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline do to keep herself busy during her pottery break?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Read a book and paint.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8759547472000122,
        "bert_recall": 0.8547959327697754,
        "bert_f1": 0.8652459979057312,
        "meteor": 0.0,
        "sbert_similarity": 0.08983378112316132
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Melanie attended about?",
      "prediction": "Not mentioned in the conversation",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8824484348297119,
        "bert_recall": 0.845316469669342,
        "bert_f1": 0.8634834885597229,
        "meteor": 0.0,
        "sbert_similarity": 0.08634094148874283
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Caroline's son on their road trip?",
      "prediction": "Not mentioned in the conversation",
      "reference": "He got into an accident",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8875336647033691,
        "bert_recall": 0.8607149124145508,
        "bert_f1": 0.8739185929298401,
        "meteor": 0.0,
        "sbert_similarity": 0.13336865603923798
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's son handle the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "He was scared but reassured by his family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8755668997764587,
        "bert_recall": 0.845429539680481,
        "bert_f1": 0.8602343797683716,
        "meteor": 0.0,
        "sbert_similarity": 0.12110541760921478
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel about her family after the accident?",
      "prediction": "They are important and mean the world to her",
      "reference": "They are important and mean the world to her",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's children handle the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "They were scared but resilient",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8781701326370239,
        "bert_recall": 0.8506824970245361,
        "bert_f1": 0.8642078042030334,
        "meteor": 0.0,
        "sbert_similarity": 0.07462230324745178
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel after the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Grateful and thankful for her family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705780506134033,
        "bert_recall": 0.8196116089820862,
        "bert_f1": 0.8443264365196228,
        "meteor": 0.0,
        "sbert_similarity": 0.012023864313960075
      }
    },
    {
      "sample_id": 0,
      "question": "What was Caroline's reaction to her children enjoying the Grand Canyon?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She was happy and thankful",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8730800151824951,
        "bert_recall": 0.8435056805610657,
        "bert_f1": 0.858038067817688,
        "meteor": 0.0,
        "sbert_similarity": 0.026393597945570946
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline do after the road trip to relax?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Went on a nature walk or hike",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8826771974563599,
        "bert_recall": 0.8326030969619751,
        "bert_f1": 0.8569092750549316,
        "meteor": 0.0,
        "sbert_similarity": 0.06870594620704651
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline love most about camping with her family?",
      "prediction": "Being present and bonding with her family",
      "reference": "Being present and bonding with her family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    }
  ]
}